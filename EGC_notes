11/11/24 Trying Code 

-Need to run every time: Pull any changes from Github
cd /uufs/chpc.utah.edu/common/home/saarman-group1/cx_ddRAD_scripts/
git pull


* fatal: detected dubious ownership in repository at '/uufs/chpc.utah.edu/common/home/saarman-group1/cx_ddRAD_scripts'
To add an exception for this directory, call:
        git config --global --add safe.directory /uufs/chpc.utah.edu/common/home/saarman-group1/cx_ddRAD_scripts
[u1055819@frisco1 cx_ddRAD_scripts]$ 


*Can't pull from github so the slurm files I have in CHPC are older ... I could manually change them in interactive session?


12/6/24

Made file for barcodes and original IDs for each sample. Only 95 in the example, so had to add my last sample in manually (agctaaga+C	CB01-I repeated from first plate)

*Using terminal on my mac? Was going to try the CHPC but that wasnt working at first, will try again. 
Going through README file trying to git pull into Cx_ddRAD scripts directory 

git pull
fatal: detected dubious ownership in repository at '/uufs/chpc.utah.edu/common/home/saarman-group1/cx_ddRAD_scripts'
To add an exception for this directory, call:

	git config --global --add safe.directory /uufs/chpc.utah.edu/common/home/saarman-group1/cx_ddRAD_scripts


12/17/2024
cd /uufs/chpc.utah.edu/common/home/saarman-group1/cx_ddRAD_scripts/
git pull

sbatch: error: invalid partition specified: saarman-shared-np
sbatch: error: Batch job submission failed: Invalid partition name specified
[u1055819@frisco1 cx_ddRAD_scripts]$ scontrol show partition saarman-shared-np
Partition saarman-shared-np not found

[u1055819@frisco1 saarman-group1]$ myallocation
Deprecation note: this program will not support the upcoming Granite cluster and beyond. Use `mychpc batch` instead.
        You have a general allocation on kingspeak. Account: saarman, Partition: kingspeak
        You have a general allocation on kingspeak. Account: saarman, Partition: kingspeak-shared
        You have a general allocation on kingspeak. Account: usu-biol4750, Partition: kingspeak
        You have a general allocation on kingspeak. Account: usu-biol4750, Partition: kingspeak-shared
        You can use preemptable mode on kingspeak. Account: owner-guest, Partition: kingspeak-guest
        You can use preemptable mode on kingspeak. Account: owner-guest, Partition: kingspeak-shared-guest
        Your group saarman does not have a general allocation on notchpeak
        You can use preemptable mode on notchpeak. Account: saarman, Partition: notchpeak-freecycle
        You can use preemptable mode on notchpeak. Account: saarman, Partition: notchpeak-shared-freecycle
        Your group usu-biol4750 does not have a general allocation on notchpeak
        You can use preemptable mode on notchpeak. Account: usu-biol4750, Partition: notchpeak-freecycle
        You can use preemptable mode on notchpeak. Account: usu-biol4750, Partition: notchpeak-shared-freecycle
        You have a general allocation on notchpeak. Account: dtn, Partition: notchpeak-dtn
        You have a general allocation on notchpeak. Account: notchpeak-shared-short, Partition: notchpeak-shared-short
        You have an owner allocation on notchpeak. Account: saarman-np, Partition: saarman-np
        You have an owner allocation on notchpeak. Account: saarman-np, Partition: saarman-shared-np
        You can use preemptable mode on notchpeak. Account: owner-guest, Partition: notchpeak-guest
        You can use preemptable mode on notchpeak. Account: owner-guest, Partition: notchpeak-shared-guest
        You have a general allocation on lonepeak. Account: saarman, Partition: lonepeak
        You have a general allocation on lonepeak. Account: saarman, Partition: lonepeak-shared
        You have a general allocation on lonepeak. Account: usu-biol4750, Partition: lonepeak
        You have a general allocation on lonepeak. Account: usu-biol4750, Partition: lonepeak-shared
        You can use preemptable mode on lonepeak. Account: owner-guest, Partition: lonepeak-guest
        You can use preemptable mode on lonepeak. Account: owner-guest, Partition: lonepeak-shared-guest


12/18/2024
git pull
sbatch 1a_process_radtags.slurm
slurm-2662574.out

1/1/2025

git pull
sbatch 5c
slurm-2761369_1.out

1/7/25
sbatch 6b_bwa - ran but gave this error 

[M::bwa_idx_load_from_disk] read 0 ALT contigs
[M::process] read 122326 sequences (40000602 bp)...
[M::process] read 122326 sequences (40000602 bp)...
slurmstepd: error: *** JOB 2761373 ON notch320 CANCELLED AT 2025-01-07T12:09:22 ***

*Not sure why maybe the old job that I cancelled is effecting this one... going to try one more time to see if the same error happens
slurm-2807933.out

1/8/25
Meeting with Norah - deleted unneccessary files - did not have to run 5c.. running 6b again to make sure it worked correctly
git pull 
sbatch 6b_bwa
Submitted batch job 2811975

Success! results in flagstat_results.tsv

1/13/2025
I am unsure where to pull from to edit 6b_refmap or 7b_populations slurm scripts... i do not see anything about stacks in ours or alkali bee project

1/17/2025
Meeting with Norah - confirmed what to do with 6b_refmap -> created refmap dummy file, checked that stacks module was available
Tried to run 6b_refmap 
Submitted batch job 2863149

Error - Parsed population map: 96 files in 1 population and 1 group.
Found 96 sample file(s).

Calling variants, genotypes and haplotypes...
  /uufs/chpc.utah.edu/sys/installdir/r8/stacks/2.64/bin/gstacks -I /uufs/chpc.utah.edu/common/home/saarman-group1/cx_ddRAD_bwa/ -M /uufs/chpc.utah.edu/common/home/saarman-group1/cx_ddRAD_scripts/cx_dummyplate2.txt -O /uufs/chpc.utah.edu/common/home/saarman-group1/cx_ddRAD_stacks


ref_map.pl: Aborted because the last command failed (1); see ref_map.log file.
Last command executed by ref_map.pl was:
  /uufs/chpc.utah.edu/sys/installdir/r8/stacks/2.64/bin/gstacks -I /uufs/chpc.utah.edu/common/home/saarman-group1/cx_ddRAD_bwa/ -M /uufs/chpc.utah.edu/common/home/saarman-group1/cx_ddRAD_scripts/cx_dummyplate2.txt -O /uufs/chpc.utah.edu/common/home/saarman-group1/cx_ddRAD_stacks


***Got an error just in sample Cb020I - it had a small BAM file so likely had poor alignment to begin with - should this sample be discarded?
samtools view /uufs/chpc.utah.edu/common/home/saarman-group1/cx_ddRAD_bwa/Cb020I.bam | head

1/19/25

Trying to diagnose the issue. A few possibilities: 

Paired-End Information:
0 + 0 paired in sequencing: This file does not contain paired-end reads; it consists of single-end reads.
*Implication: Ensure your pipeline parameters (e.g., in gstacks) are compatible with single-end data

Going to try excluding sample Cb020I from the analysis it has super low reads in BAM file 
samtools flagstat Cb020I.bam 
20 + 0 in total (QC-passed reads + QC-failed reads)
9 + 0 primary
11 + 0 secondary
0 + 0 supplementary
0 + 0 duplicates
0 + 0 primary duplicates
14 + 0 mapped (70.00% : N/A)
3 + 0 primary mapped (33.33% : N/A)
0 + 0 paired in sequencing
0 + 0 read1
0 + 0 read2
0 + 0 properly paired (N/A : N/A)
0 + 0 with itself and mate mapped
0 + 0 singletons (N/A : N/A)
0 + 0 with mate mapped to a different chr
0 + 0 with mate mapped to a different chr (mapQ>=5)

mkdir /uufs/chpc.utah.edu/common/home/saarman-group1/cx_ddRAD_bwa/excluded
mv /uufs/chpc.utah.edu/common/home/saarman-group1/cx_ddRAD_bwa/{Cb020I.bam,Cb012E.bam,Cb018F.bam,Cb017H.bam,Cb015D.bam,Cb014G.bam,Cb016B.bam} /uufs/chpc.utah.edu/common/home/saarman-group1/cx_ddRAD_bwa/excluded/


Removed 7 files total - 
Cb020I.bam	
Cb018F.bam	
Cb017H.bam	
Cb012E.bam	
Cb015D.bam	
Cb014G.bam	
Cb016B.bam

made new output file cx_ddRAD_stacks_filtered
sbatch 6b_refmap.slurm 
Submitted batch job 2882783

sbatch 7b
Submitted batch job 2882814

1/22/25
Meeting with Norah - adding 8b bcftools step for Allele balance, can remove 8c_vcf
Changed 8a to make filters harder and softer options

sbatch 8a_plink.slurm 
Submitted batch job 2906973

*error wrong file name, had to add vcf module, 

1/23/25
Split 8plink into steps a and b to convert plink to vcf
Submitted batch job 2922703

After filtering, kept 89 out of 89 Individuals
Outputting VCF file...
After filtering, kept 13597 out of a possible 36024 Sites
Run Time = 2.00 seconds

Submitted batch job 2923700

[filter.c:2933 filters_init1] Error: the tag "AB" is not defined in the VCF header
Start time : 14:08:59

1/29/25
Troubleshooting AB balance as Allele Depth calculation

bcftools view -v snps,indels /uufs/chpc.utah.edu/common/home/saarman-group1/cx_ddRAD_filtered/cx_ddRAD_plate2_g70m70g50_filtered.recode.vcf -Ou | bcftools filter -i 'GT="0/1"' -Ov |
 head -n 20

##fileformat=VCFv4.2
##FILTER=<ID=PASS,Description="All filters passed">
##fileDate=20250119
##source="Stacks v2.64"
##contig=<ID=scaffold_1,length=219626240>
##contig=<ID=scaffold_10,length=242402>
##contig=<ID=scaffold_100,length=23821>
##contig=<ID=scaffold_101,length=23765>
##contig=<ID=scaffold_102,length=23701>
##contig=<ID=scaffold_103,length=23662>
##contig=<ID=scaffold_104,length=23514>
##contig=<ID=scaffold_105,length=22920>
##contig=<ID=scaffold_106,length=22817>
##contig=<ID=scaffold_107,length=22581>
##contig=<ID=scaffold_108,length=22538>
##contig=<ID=scaffold_109,length=21978>
##contig=<ID=scaffold_110,length=21833>
##contig=<ID=scaffold_111,length=21412>
##contig=<ID=scaffold_112,length=21226>
##contig=<ID=scaffold_113,length=21032>

Since this prints VCF lines it is likely that bcftools is working but has issues writing to the output file.

changed concat line to - bcftools concat -a -O v -o ${out_file}

sbatch 8b
Submitted batch job 2985823
*Still no output

1/30/25
Tried something chat recommended made a new troubleshooting slurm script in 8tb_bcftools

sbatch 8tb_bcftools.slurm 
Submitted batch job 2986338

Still error with outputs because of big files not being unzipped? adding lines into 8tb file

sbatch 8tb_bcftools.slurm 
Submitted batch job 2987751
AGAIN sbatch 8tb_bcftools.slurm
Submitted batch job 2987752
AGAIn sbatch 8tb_bcftools.slurm 
Submitted batch job 2987756
trying again sbatch 8tb_bcftools.slurm 
Submitted batch job 2987947
sbatch 8tb_bcftools.slurm
Submitted batch job 2987958

NOTHING WORKED chat was annoying and too complicated
Going back to original 8b to see what the error was again

sbatch 8b_bcftools.slurm 
Submitted batch job 2990887

Something wrong with concatenate line ?

sbatch 8b_bcftools.slurm 
Submitted batch job 2991066

Met with Norah - trying her recommendation combining NPS notes and 8b

sbatch 8tsb_bcftools.slurm 
Submitted batch job 2991581

Same thing in output slurm file.... is this ok? do I need to manually gunzip files?
Start time : 14:48:09

About:   Concatenate or combine VCF/BCF files. All source files must have the same sample
         columns appearing in the same order. The program can be used, for example, to
         concatenate chromosome VCFs into one VCF, or combine a SNP VCF and an indel
         VCF into one. The input files must be sorted by chr and position. The files
         must be given in the correct order to produce sorted VCF on output unless
         the -a, --allow-overlaps option is specified. With the --naive option, the files
         are concatenated without being recompressed, which is very fast.
Usage:   bcftools concat [options] <A.vcf.gz> [<B.vcf.gz> [...]]

Options:
   -a, --allow-overlaps           First coordinate of the next file can precede last record of the current file.
   -c, --compact-PS               Do not output PS tag at each site, only at the start of a new phase set block.
   -d, --rm-dups STRING           Output duplicate records present in multiple files only once: <snps|indels|both|all|exact>
   -D, --remove-duplicates        Alias for -d exact
   -f, --file-list FILE           Read the list of files from a file.
   -l, --ligate                   Ligate phased VCFs by matching phase at overlapping haplotypes
       --ligate-force             Ligate even non-overlapping chunks, keep all sites
       --ligate-warn              Drop sites in imperfect overlaps
       --no-version               Do not append version and command line to the header
   -n, --naive                    Concatenate files without recompression, a header check compatibility is performed
       --naive-force              Same as --naive, but header compatibility is not checked. Dangerous, use with caution.
   -o, --output FILE              Write output to a file [standard output]
   -O, --output-type u|b|v|z[0-9] u/b: un/compressed BCF, v/z: un/compressed VCF, 0-9: compression level [v]
   -q, --min-PQ INT               Break phase set if phasing quality is lower than <int> [30]
   -r, --regions REGION           Restrict to comma-separated list of regions
   -R, --regions-file FILE        Restrict to regions listed in a file
       --regions-overlap 0|1|2    Include if POS in the region (0), record overlaps (1), variant overlaps (2) [1]
       --threads INT              Use multithreading with <int> worker threads [0]
   -v, --verbose 0|1              Set verbosity level [1]

The FORMAT tag AD can have multiple subfields, run as AD[sample:subfield]
Pipeline completed successfully
End time : 14:48:11

1/31/25 
Trying a new concat strategy 

sbatch 8tsb_bcftools.slurm 
Submitted batch job 2997158

It was working! Just labeled differently than I thought oops ( -o ${final_filtered} )

Going to rerun with new output file path. 

Norah writing a new PLINK and pop file to check the summary statistics of the final filtered SNPs

2/4/25
Successfully got filtered snps file in correct directory -Submitted batch job 3030008
Running 9aplink 
Submitted batch job 3030182

Things seemed to work but at the end of slurm file it says - Error: Stacks populations summary statistics were not generated.

2/13/25 Meeting with Norah 
It did work! The error was generated because that specific file wasnt there was another output file with a different name. 


3/7/25 

ddrad Analysis for Plate 2 
-Notes form Norah: try running 9bplink, not sure I ran it before or we added it in the meeting and I forgot. 


3/9/25 

Tried to run 9bplink
-created a few files in the  cx_ddrad_filtered folder but nothing that seems super helpful.. 

SLURM file output
slurm-3513307.out

The following have been reloaded with a version change:
  1) plink/2.0 => plink/1.90

Converting VCF to PLINK format...
PLINK v1.90b5.3 64-bit (21 Feb 2018)           www.cog-genomics.org/plink/1.9/
(C) 2005-2018 Shaun Purcell, Christopher Chang   GNU General Public License v3
Logging to /uufs/chpc.utah.edu/common/home/saarman-group1/cx_ddRAD_filtered/cx_ddRAD_plate2_g70m70g50_filtered_snps_indels.log.
Options in effect:
  --make-bed
  --out /uufs/chpc.utah.edu/common/home/saarman-group1/cx_ddRAD_filtered/cx_ddRAD_plate2_g70m70g50_filtered_snps_indels
  --vcf /uufs/chpc.utah.edu/common/home/saarman-group1/cx_ddRAD_filtered/cx_ddRAD_plate2_g70m70g50_filtered_snps_indels.vcf

1031534 MB RAM detected; reserving 515767 MB for main workspace.

Error: Invalid chromosome code 'scaffold_1' on line 241 of .vcf file.
(Use --allow-extra-chr to force it to be accepted.)
Warning: PLINK file conversion failed.


Trying one more time changing the module plink download to version 2.0

Submitted batch job 3513313

Converting VCF to PLINK format...
/uufs/notchpeak.peaks/sys/var/slurm/slurmd.spool/notch320//job3513313/slurm_script: line 39: plink: command not found
Warning: PLINK file conversion failed.

trying to manually load module 2.0 
same error - 

Converting VCF to PLINK format...
/uufs/notchpeak.peaks/sys/var/slurm/slurmd.spool/notch320//job3513318/slurm_script: line 39: plink: command not found
Warning: PLINK file conversion failed.


07/01/2025
Step 5b - perl/slurm script for plate 3
5048862 saarman-s bwa-mem2 u1055819  R       0:02      1 notch320



09/03/2025
Outputs found in cx_ddRAD_bwa for plate 3 - in directory now, should move to 06-25 folder when finished...

Going to run 6b-bwaflagstat
sbatch 6b_bwa_flagstat.slurm 
Submitted batch job 6014309
Will save as plate3-flagstat.tsv

sbatch 6b_refmap.slurm 
Submitted batch job 6017407 saarman-s   stacks u1055819  R       2:06      1 notch320

sbatch 7b_populations.slurm 
Submitted batch job 6018601 -done!


9/04/25
sbatch 8a1_plink.slurm 
Submitted batch job 6034528

sbatch 8a2_plink2vcf.slurm
Submitted batch job 6034575

sbatch 8b_bcftools.slurm 
Submitted batch job 6034713
-did not create output
-edited file to make temp files and make output 

sbatch 8b_bcftools.slurm 
Submitted batch job 6034999
-still no output, 

sbatch 8b_bcftools.slurm 
Submitted batch job 6035024

-Still not working, goingg to go back to original file

sbatch 8b_bcftools.slurm 
Submitted batch job 6062359

*maybe need to use 8bfinal_bcftoolsABfilter.slurm and not use 8bcftools at all?

9/17/2025

Trying 8bfinal .... trying to figure out the difference between the two files.

got 8bfinal to work for plate 3!!!!		

9/18/2025
Deciding which file to use for sumstats
Option 1: cx_ddRAD_plate3_g70m70g50_filtered_snps_indels.vcf = out_file (SNPs + indels)
Contains indels and SNPs.
Stacks populations is primarily designed for SNP datasets. Including indels may cause issues, or at least add noise, since summary stats (e.g., heterozygosity, Fst) are usually based on SNPs only.
Might inflate your statistics or complicate filtering later.

Option 2: cx_ddRAD_plate3_g70m70g50_finalfiltered_snps_noindels.vcf = final_filtered (SNP-only, stricter)
Contains only SNPs.
Extra filtering ensures you’re working with high-confidence variants (balanced heterozygotes, reasonable allele frequencies).
This is exactly the kind of dataset Stacks is optimized for when generating summary stats like π, Ho, He, Fst, etc.

9a 
Using option to which is filtered for no indels
-made pop map
slurm-6428315.out

9b
slurm-6428550.out

success! had to add flag -allow-extra-chromosomes- and change to plink 1.9 for some reason but it worked!


*now to find out how to compare plates 2 and 3 
Using R Studio Server - looking at sumstats files. trying to make plots that compare the loci and pop summaries?

abplate3_popsummary					Population-level				AB3 population summary (7 pops × 5 vars)
cbplateZ_popsummary					Population-level				CB2 population summary (10 pops × 5 vars)
absumstats_individuals				Locus/individual-level			AB3 loci × individuals summary (216,278 rows × 21 vars)
cbsumstats_individuals				Locus/individual-level			CB2 loci × individuals summary (103,632 rows × 21 vars)
combinedloci_summary				Locus/individual-level			Merged AB3+CBZ (103,632 rows × 37 vars)
combinedpop_summary					Population-level				Merged AB3+CBZ (10 rows × 9 vars)




Nov 5th, 2025

Going to go back to step 6 - combine both plate 2 and 3 so we can actually compare
-thinking i will combine everything into the cx-ddrad-bwa folder. can tell which run by the date!
-also moving all that were in the excluded folder back into the main directory.. going to run the flagstat step 6 again but Norah advised to not remove too many early on.
-here is the list that were removed previously that i am going to put back in the main folder
Ab014C.bam	19.77 kB	7/1/2025 1:03:43 PM 
Ab016J.bam	74.44 kB	7/1/2025 1:11:29 PM
Ab017A.bam	33.15 kB	7/1/2025 1:11:31 PM
Ab017B.bam	85.05 kB	7/1/2025 1:11:34 PM
Ab019B.bam	7.10 kB		7/1/2025 1:11:39 PM
Ab023B.bam	24.95 kB	7/1/2025 1:37:19 PM
Ab024E.bam	23.53 kB	7/1/2025 1:49:23 PM
Ab032E.bam	304.73 kB	7/1/2025 2:07:29 PM
Cb003M.bam	11.12 kB	7/1/2025 2:13:28 PM
Cb012E.bam	10.60 kB	12/20/2024 11:58:30 AM
Cb014G.bam	15.89 kB	12/20/2024 12:08:13 PM
Cb015D.bam	14.03 kB	12/20/2024 12:13:14 PM
Cb016B.bam	32.92 kB	12/20/2024 12:16:38 PM
Cb017H.bam	7.96 kB		12/20/2024 12:30:57 PM
Cb018F.bam	6.89 kB		12/20/2024 12:36:02 PM
Cb020I.bam	5.47 kB		12/20/2024 12:50:40 PM
Cb020L.bam	25.53 kB	7/1/2025 2:18:16 PM
Cb020M.bam	46.44 kB	7/1/2025 2:18:18 PM


NOv 7th
ok did 6a again, now 6b - 

running into error with Cb020I , checked and it only has 20 reads! 
-Trying to decide if it is worth going back and trying to redo this one sample or exclude it move on with the rest, then go back if we want..
-Ok decided, moved Cb020I to cx_ddRAD_bwa/excluded directory. going to try to rerun and see if everyone else works now.

*Everything else worked great! Output files in /cx-ddrad-stacks-filtered

Running 7-populations now!

Logging to '/uufs/chpc.utah.edu/common/home/saarman-group1/cx_ddRAD_populations_output/populations.log'.
Locus/sample distributions will be written to '/uufs/chpc.utah.edu/common/home/saarman-group1/cx_ddRAD_populations_output/populations.log.distribs'.
populations parameters selected:
  Percent samples limit per population: 0.25
  Locus Population limit: 1
  Percent samples overall: 0
  Minor allele frequency cutoff: 0
  Maximum observed heterozygosity cutoff: 1
  Applying Fst correction: none.
  Pi/Fis kernel smoothing: off
  F-stats kernel smoothing: off
  Bootstrap resampling: off

Parsing population map...
The population map contained 186 samples, 1 population(s), 1 group(s).
Working on 186 samples.
Working on 1 population(s):
    All: Ab001F, Ab001G, Ab005A, Ab006A, Ab006B, Ab006C, Ab006D, Ab006E, Ab006F, Ab007A, Ab007B, Ab007C, Ab007D, Ab007E, Ab007F, Ab007G, 
         Ab007H, Ab011A, Ab012A, Ab012B, Ab012C, Ab013A, Ab014A, Ab014B, Ab014C, Ab014D, Ab014E, Ab014F, Ab015A, Ab016F, Ab016G, Ab016H, 
         Ab016I, Ab016J, Ab017A, Ab017B, Ab019A, Ab019B, Ab021A, Ab021B, Ab021C, Ab021D, Ab021E, Ab021F, Ab021G, Ab022A, Ab022B, Ab022C, 
         Ab022D, Ab022E, Ab023A, Ab023B, Ab023C, Ab023D, Ab023E, Ab024A, Ab024B, Ab024C, Ab024D, Ab024E, Ab024F, Ab025A, Ab027A, Ab027B, 
         Ab030A, Ab030B, Ab030C, Ab030D, Ab031A, Ab031B, Ab031C, Ab032A, Ab032B, Ab032C, Ab032D, Ab032E, Cb001Ir, Cb001K, Cb001L, 
         Cb001M, Cb001N, Cb002H, Cb003K, Cb003L, Cb003M, Cb003N, Cb003O, Cb003P, Cb011L, Cb012A, Cb012B, Cb012C, Cb012D, Cb012E, Cb012F, 
         Cb012G, Cb012H, Cb012I, Cb012J, Cb012K, Cb013A, Cb013B, Cb013C, Cb013D, Cb013E, Cb013F, Cb013G, Cb013H, Cb013I, Cb013J, Cb013K, 
         Cb014A, Cb014B, Cb014C, Cb014D, Cb014E, Cb014F, Cb014G, Cb014H, Cb014I, Cb014J, Cb014K, Cb015A, Cb015B, Cb015C, Cb015D, Cb015E, 
         Cb015F, Cb015G, Cb015H, Cb015I, Cb015J, Cb016A, Cb016B, Cb016C, Cb016D, Cb016E, Cb016F, Cb016G, Cb016H, Cb016I, Cb016J, Cb017A, 
         Cb017B, Cb017C, Cb017D, Cb017E, Cb017F, Cb017G, Cb017H, Cb017I, Cb017J, Cb018A, Cb018B, Cb018C, Cb018D, Cb018E, Cb018F, Cb018G, 
         Cb018H, Cb018I, Cb018J, Cb018K, Cb019A, Cb019B, Cb019C, Cb019D, Cb019E, Cb019F, Cb019G, Cb019H, Cb019I, Cb019J, Cb01IR, Cb020A, 
         Cb020B, Cb020C, Cb020D, Cb020E, Cb020F, Cb020G, Cb020H, Cb020J, Cb020K, Cb020L, Cb020M
Working on 1 group(s) of populations:
    defaultgrp: All

SNPs and calls will be written in VCF format to '/uufs/chpc.utah.edu/common/home/saarman-group1/cx_ddRAD_populations_output/populations.snps.vcf'
Polymorphic sites in PLINK format will be written to '/uufs/chpc.utah.edu/common/home/saarman-group1/cx_ddRAD_populations_output/populations.plink.ped'
Raw haplotypes will be written to '/uufs/chpc.utah.edu/common/home/saarman-group1/cx_ddRAD_populations_output/populations.haplotypes.tsv'
Population-level summary statistics will be written to '/uufs/chpc.utah.edu/common/home/saarman-group1/cx_ddRAD_populations_output/populations.sumstats.tsv'
Population-level haplotype summary statistics will be written to '/uufs/chpc.utah.edu/common/home/saarman-group1/cx_ddRAD_populations_output/populations.hapstats.tsv'

Processing data in batches:
  * load a batch of catalog loci and apply filters
  * compute SNP- and haplotype-wise per-population statistics
  * write the above statistics in the output files
  * export the genotypes/haplotypes in specified format(s)
More details in '/uufs/chpc.utah.edu/common/home/saarman-group1/cx_ddRAD_populations_output/populations.log.distribs'.

Now processing...
scaffold_1 
scaffold_10 
scaffold_101 
scaffold_104 
scaffold_112 
scaffold_113 
scaffold_115 
scaffold_12 
scaffold_121 
scaffold_13 
scaffold_145 
scaffold_148 
scaffold_15 
scaffold_151 
scaffold_16 
scaffold_160 
scaffold_164 
scaffold_17 
scaffold_18 
scaffold_184 
scaffold_186 
scaffold_19 
scaffold_196 
scaffold_2 
scaffold_211 
scaffold_213 
scaffold_215 
scaffold_216 
scaffold_222 
scaffold_225 
scaffold_235 
scaffold_24 
scaffold_243 
scaffold_245 
scaffold_249 
scaffold_25 
scaffold_250 
scaffold_251 
scaffold_257 
scaffold_261 
scaffold_265 
scaffold_3 
scaffold_33 
scaffold_34 
scaffold_38 
scaffold_4 
scaffold_47 
scaffold_5 
scaffold_58 
scaffold_59 
scaffold_62 
scaffold_69 
scaffold_7 
scaffold_75 
scaffold_78 
scaffold_8 
scaffold_9 

Removed 451193 loci that did not pass sample/population constraints from 486620 loci.
Kept 35427 loci, composed of 11785396 sites; 218996 of those sites were filtered, 33580 variant sites remained.
    11574699 genomic sites, of which 208102 were covered by multiple loci (1.8%).
Mean genotyped sites per locus: 332.48bp (stderr 0.08).

Population summary statistics (more detail in populations.sumstats_summary.tsv):
  All: 72.068 samples per locus; pi: 0.1236; all/variant/polymorphic sites: 11778665/33580/33580; private alleles: 0
Populations is done.


11/9/2025

8a done 
8b_bcf filter causing issues. troubleshooting slurm7976537
tried to fix... idk what the issue was
moving on to 8b_AB filter = 7976595
*didnt work either. something about zipped files?

11/12/25 
trying 8b_bcf again. i think it is something to do with file types like its not reading the vcf files because its bcf? can I just load both modules or is there a way to transform...

says this but the output file is empty: 

Options:
   -a, --allow-overlaps           First coordinate of the next file can precede last record of the current file.
   -c, --compact-PS               Do not output PS tag at each site, only at the start of a new phase set block.
   -d, --rm-dups STRING           Output duplicate records present in multiple files only once: <snps|indels|both|all|exact>
   -D, --remove-duplicates        Alias for -d exact
   -f, --file-list FILE           Read the list of files from a file.
   -l, --ligate                   Ligate phased VCFs by matching phase at overlapping haplotypes
       --ligate-force             Ligate even non-overlapping chunks, keep all sites
       --ligate-warn              Drop sites in imperfect overlaps
       --no-version               Do not append version and command line to the header
   -n, --naive                    Concatenate files without recompression, a header check compatibility is performed
       --naive-force              Same as --naive, but header compatibility is not checked. Dangerous, use with caution.
   -o, --output FILE              Write output to a file [standard output]
   -O, --output-type u|b|v|z[0-9] u/b: un/compressed BCF, v/z: un/compressed VCF, 0-9: compression level [v]
   -q, --min-PQ INT               Break phase set if phasing quality is lower than <int> [30]
   -r, --regions REGION           Restrict to comma-separated list of regions
   -R, --regions-file FILE        Restrict to regions listed in a file
       --regions-overlap 0|1|2    Include if POS in the region (0), record overlaps (1), variant overlaps (2) [1]
       --threads INT              Use multithreading with <int> worker threads [0]
   -v, --verbose 0|1              Set verbosity level [1]

The FORMAT tag AD can have multiple subfields, run as AD[sample:subfield]
Pipeline completed successfully
End time : 10:02:52

*Went back and found out i was missing 5 pops!!! Ab01A-E Adding them back into popmap and rerunning steps 6b and 7

sbatch 6b_popmap.slurm 
Submitted batch job 8329553
output should be in /stacksfiltered
8329553 saarman-s   stacks u1055819  R      1:01:42       1 notch320 - taking a while


11/12/25 
Had to contact help desk because my group path disappeared?! Made a symlink so now I  have this as a path to the saarman group but will need to remember to change all the scripts to this :(
here is how i created symlink in terminal: 
ln -s /uufs/chpc.utah.edu/common/home/saarman-group1/ saarman-group

so now my path is this:

/uufs/chpc.utah.edu/common/home/u1055819/saarman-group/

11/13/25 
Ok moving on! Onto 7pop
Parsing population map...
The population map contained 191 samples, 1 population(s), 1 group(s).
Working on 191 samples.
Working on 1 population(s):
    All: Ab001A, Ab001B, Ab001C, Ab001D, Ab001E, Ab001F, Ab001G, Ab005A, Ab006A, Ab006B, Ab006C, Ab006D, Ab006E, Ab006F, Ab007A, Ab007B, 
         Ab007C, Ab007D, Ab007E, Ab007F, Ab007G, Ab007H, Ab011A, Ab012A, Ab012B, Ab012C, Ab013A, Ab014A, Ab014B, Ab014C, Ab014D, Ab014E, 
         Ab014F, Ab015A, Ab016F, Ab016G, Ab016H, Ab016I, Ab016J, Ab017A, Ab017B, Ab019A, Ab019B, Ab021A, Ab021B, Ab021C, Ab021D, Ab021E, 
         Ab021F, Ab021G, Ab022A, Ab022B, Ab022C, Ab022D, Ab022E, Ab023A, Ab023B, Ab023C, Ab023D, Ab023E, Ab024A, Ab024B, Ab024C, Ab024D, 
         Ab024E, Ab024F, Ab025A, Ab027A, Ab027B, Ab030A, Ab030B, Ab030C, Ab030D, Ab031A, Ab031B, Ab031C, Ab032A, Ab032B, Ab032C, Ab032D, 
         Ab032E, Cb001Ir, Cb001K, Cb001L, Cb001M, Cb001N, Cb002H, Cb003K, Cb003L, Cb003M, Cb003N, Cb003O, Cb003P, Cb011L, Cb012A, 
         Cb012B, Cb012C, Cb012D, Cb012E, Cb012F, Cb012G, Cb012H, Cb012I, Cb012J, Cb012K, Cb013A, Cb013B, Cb013C, Cb013D, Cb013E, Cb013F, 
         Cb013G, Cb013H, Cb013I, Cb013J, Cb013K, Cb014A, Cb014B, Cb014C, Cb014D, Cb014E, Cb014F, Cb014G, Cb014H, Cb014I, Cb014J, Cb014K, 
         Cb015A, Cb015B, Cb015C, Cb015D, Cb015E, Cb015F, Cb015G, Cb015H, Cb015I, Cb015J, Cb016A, Cb016B, Cb016C, Cb016D, Cb016E, Cb016F, 
         Cb016G, Cb016H, Cb016I, Cb016J, Cb017A, Cb017B, Cb017C, Cb017D, Cb017E, Cb017F, Cb017G, Cb017H, Cb017I, Cb017J, Cb018A, Cb018B, 
         Cb018C, Cb018D, Cb018E, Cb018F, Cb018G, Cb018H, Cb018I, Cb018J, Cb018K, Cb019A, Cb019B, Cb019C, Cb019D, Cb019E, Cb019F, Cb019G, 
         Cb019H, Cb019I, Cb019J, Cb01IR, Cb020A, Cb020B, Cb020C, Cb020D, Cb020E, Cb020F, Cb020G, Cb020H, Cb020J, Cb020K, Cb020L, Cb020M

Yay - correct amount of samples for 2 plates minus Cb020I that had ~20 reads and i removed manually

Now for 8a1_plink - all looks good! filtering options available. 34018 variants before filtering. going to go ahead with the hard filter for Geno soft filter for Mind to retain the most individuals and the least SNPs so we know they are high quality and less to deal with
*added info to Cx_ddRAD_plink google sheet
Now for 8a2 = slurm output 8435992

VCFtools - 0.1.15
(C) Adam Auton and Anthony Marcketta 2009

Parameters as interpreted:
	--vcf /uufs/chpc.utah.edu/common/home/u1055819/saarman-group/cx_ddRAD_populations_output/populations.snps.vcf
	--keep populations.snps_g70m70g20.nosex
	--out cx_ddRAD_combined2-3_g70m70g20_filtered.recode
	--positions populations.snps_g70m70g20.pos
	--recode

Keeping individuals in 'keep' list
After filtering, kept 146 out of 191 Individuals
Outputting VCF file...
After filtering, kept 1636 out of a possible 34018 Sites
Run Time = 0.00 seconds

*saved as recode.recode.vcf maybe should change? i just went with it this time

Now for 8b! 8436128 *chmod errors potentially. going to comment out the first chmod line of code and rerun
